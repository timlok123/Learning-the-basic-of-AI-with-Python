{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvGCns8CTwOv"
      },
      "source": [
        "# Ch6 - Possible problem in AI training and Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_P5zybSTiz5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(List the possible problems & solution encountered during training AI network)"
      ],
      "metadata": {
        "id": "PtMnD1cirZps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network training"
      ],
      "metadata": {
        "id": "N19bdilotIbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "GSzafuwbsswx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following of the notebook, we are using the *Iris flower dataset* for training.\n",
        "(Explanation on the data, the components of data, no of data)"
      ],
      "metadata": {
        "id": "gPOWd7jvrqt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The structure of the AI model, activation function & output layer function in between, optimization methods and loss function, batch size, test case,  )"
      ],
      "metadata": {
        "id": "-Qz9yUFrsGG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "SnXijPwNs1zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the steps of data preparation,\n",
        "\n",
        "1. Load the data from *sklearn* module\n",
        "2. Normalization\n",
        "3. Convert the label for training (Understand why do we need to do this)\n",
        "4. Split the train and test case"
      ],
      "metadata": {
        "id": "rO9aIlfztqo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris_data = datasets.load_iris()\n",
        "input_data = iris_data.data\n",
        "correct = iris_data.target\n",
        "\n",
        "no_of_data = len(correct)"
      ],
      "metadata": {
        "id": "MZ3vgKrMs-L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 data & take a look of the data\n",
        "print(input_data[:10,])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaWjDCTHvP0W",
        "outputId": "cf996a4b-d888-41ea-c2f0-0294ff199e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "mean_col = np.mean(input_data,axis=0)\n",
        "std_col = np.std(input_data,axis=0)\n",
        "normalized_data = (input_data - mean_col)/(std_col)"
      ],
      "metadata": {
        "id": "lC40MQKwvLO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn the `correct` to one-hot labelling (Add the graph of one-hot labelling scheme & why do we need to turn it to one-hot)"
      ],
      "metadata": {
        "id": "plCK2MM6waD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the `correct` to one-hot labelling\n",
        "correct_data = np.zeros((no_of_data,3),dtype=int)\n",
        "\n",
        "# TODO: Figure out a way to make use the vectorization\n",
        "for i in range(no_of_data):\n",
        "    correct_data[i][correct[i]] = 1\n"
      ],
      "metadata": {
        "id": "HxVI3Zt_wGDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data to test & learn case\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_train, intpu_test, correct_train, correct_test = train_test_split(\n",
        "    normalized_data,\n",
        "    correct_data,\n",
        "    test_size=no_of_data//2\n",
        ")"
      ],
      "metadata": {
        "id": "_bQF7hsezN8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network structure"
      ],
      "metadata": {
        "id": "paMrEpoas4OD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define the hyperparameters & initial vaules for the network\n",
        "2. Define the layer structure as object\n",
        "3. Forward and back propagation definition\n"
      ],
      "metadata": {
        "id": "jqIfjwqz0sDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nework neuron\n",
        "n_in = 4 # since we get 4 features for each x\n",
        "n_mid = 25\n",
        "n_out = 3\n",
        "\n",
        "eta = 0.01      # learning rate for SGD\n",
        "epoch = 1000    # No of training times\n",
        "batch_size = 8\n",
        "\n",
        "interval = 100  # Showing the training accuracy every (interval) times"
      ],
      "metadata": {
        "id": "VYxMBcd9rp0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ReLu definition, Softmax expression)"
      ],
      "metadata": {
        "id": "SBGfQw5-3n0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the layer structure & methods\n",
        "\n",
        "\n",
        "# Define the common properities in BaseLayer\n",
        "class BaseLayer:\n",
        "  def __init__(self,n_upper,n, wb_width = 0.1): # n_upper - no of neurons in last layer, n - no of neurons\n",
        "    self.w = wb_width * np.random.randn(n_upper, n)\n",
        "    self.b = wb_width * np.random.randn(n)\n",
        "\n",
        "  def update(self,eta):\n",
        "    self.w -= eta * self.grad_w\n",
        "    self.b_-= eta * self.grad_b\n",
        "\n",
        "# Middle layer\n",
        "class MiddleLayer(BaseLayer):\n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    self.u = np.dot(x, self.w) + self.b\n",
        "    self.y = np.where(self.u<=0, 0, self.u) # ReLu activation function ]\n",
        "\n",
        "  def backward(self,grad_y):\n",
        "    delta = grad_y * np.where(self.u>0, 1, 0) # grad_y * derivative of ReLu\n",
        "\n",
        "    self.grad_w = np.dot(self.x.T, delta)\n",
        "    self.grad_b = np.sum(delta, axis=0)\n",
        "\n",
        "    self.grad_x = np.dot(delta, self.w.T)\n",
        "\n",
        "# Output layer\n",
        "class OutputLayer(BaseLayer):\n",
        "  def forward(self,x):\n",
        "    self.x = x\n",
        "    self.u = np.dot(x,self.w) + self.b\n",
        "    self.y = np.exp(self.u)/(np.sum(self.u,axis=1,keepdims=True))\n",
        "\n",
        "  def backward(self,t):\n",
        "    delta = self.y - t\n",
        "\n",
        "    self.grad_w = np.dot(self.x.T, delta)\n",
        "    self.grad_b = np.sum(delta, axis=0)\n",
        "\n",
        "    self.grad_x = np.dot(delta, self.w.T)\n"
      ],
      "metadata": {
        "id": "JSW2pfrz2H47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network structure\n",
        "\n",
        "middle_layer_1 = MiddleLayer(n_in, n_mid)\n",
        "middle_layer_2 = MiddleLayer(n_mid, n_mid)\n",
        "output_layer = OutputLayer(n_mid,n_out)"
      ],
      "metadata": {
        "id": "CV4x38gc87EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training procedures"
      ],
      "metadata": {
        "id": "DYbvkgKNs_7f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JwKjFjhrY9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}